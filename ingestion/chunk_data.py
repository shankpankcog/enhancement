# Databricks notebook source
from io import BytesIO
import fitz 
import pandas as pd
from pyspark.sql.functions import col, explode, pandas_udf
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pyspark.sql.functions import col, udf, length, pandas_udf, explode
from pyspark.sql.types import ArrayType, StringType
from mlflow.deployments import get_deploy_client
import pyspark.sql.functions as F
from pyspark.sql.functions import input_file_name
from databricks.vector_search.client import VectorSearchClient
from databricks.sdk import WorkspaceClient
import databricks.sdk.service.catalog as c
from langchain.vectorstores import DatabricksVectorSearch
from langchain.embeddings import DatabricksEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.chat_models import ChatDatabricks
from pyspark.sql.functions import regexp_extract
import time
import os
from io import StringIO
import pandas as pd
import pymupdf
import pymupdf4llm
from pyspark.sql.functions import col
import datetime
from pyspark.sql.functions import pandas_udf
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType
from delta.tables import DeltaTable
import ast

# COMMAND ----------

catalog_name = "pdm-pdm-dl-quality-docs-genai-dev"
schema_name = "gvault_test"

docs_metadata_table=f"`{catalog_name}`.{schema_name}.veeva_qdms_docs_metadata"
doc_pages_table_name = f"`{catalog_name}`.`{schema_name}`.veeva_qdms_docs_pages"
embeddings_table_name = f"`{catalog_name}`.`{schema_name}`.veeva_qdms_docs_embeddings"

# COMMAND ----------

# spark.sql(f"DROP TABLE IF EXISTS {embedding_table_name}");
spark.sql(f"""CREATE TABLE IF NOT EXISTS {embeddings_table_name} (
  chunk_id BIGINT GENERATED BY DEFAULT AS IDENTITY,
  metadata_id BIGINT,
  chunk_text STRING,
  document_page_number INT,
  document_total_pages INT,
  chunking_status STRING,
  embedding ARRAY<FLOAT>,
  embedding_status STRING,
  audit__created_date TIMESTAMP DEFAULT current_timestamp(),
  audit__updated_date TIMESTAMP DEFAULT current_timestamp()
) LOCATION 's3://gilead-edp-pdm-dev-us-west-2-pdm-dl-quality-docs-genai/{embeddings_table_name.split(".")[-1]}/' 
TBLPROPERTIES('delta.feature.allowColumnDefaults' = 'supported')""");

# COMMAND ----------

# def chunk_text(pages) -> tuple:
#     try:
#         return ("COMPLETED", pages)
#     except Exception as e:
#         return ("FAILED", [{"page_number": -1, "page_text": "", "total_pages": -1}])
    
# # array<struct<page_number:int,page_text:string,total_pages:int>>
# schema = StructType([
#     StructField("status", StringType(), True),
#     StructField("pages", ArrayType(
#         StructType([
#             StructField("page_number", IntegerType(), True),
#             StructField("page_text", StringType(), True),
#             StructField("total_pages", IntegerType(), True)
#         ])
#     ), True)
# ])

# @pandas_udf(schema)
# def chunk_text_udf(text_column: pd.Series) -> pd.Series:
#     results = text_column.apply(lambda x: chunk_text(x) if x is not None else [])
#     # separate into two Series
#     statuses = results.map(lambda tpl: tpl[0])
#     pages_arr = results.map(lambda tpl: tpl[1])
#     # ***MUST*** return a DataFrame when the returnType is StructType
#     return pd.DataFrame({
#         "status": statuses,
#         "pages": pages_arr
#     })


# COMMAND ----------

def chunk_data(data_df, embedding_table_name):
    # df_repartitioned = data_df.repartition(50)
    chunk_df = data_df.withColumn("chunk_text_dict", explode(col("pdf_text"))).withColumn("chunking_status", F.lit("COMPLETED"))

    return chunk_df.withColumn("embedding", F.lit(None)).withColumn("embedding_status", F.lit(None)).select("metadata_id",col("chunk_text_dict.page_text").alias("chunk_text"), col("chunk_text_dict.page_number").alias("document_page_number"),col("chunk_text_dict.total_pages").alias("document_total_pages"), "chunking_status", "embedding", "embedding_status")
    


# COMMAND ----------

def upsert_chunk_data(data_df, embeddings_table_name):
    print(f"updating embedding table {embeddings_table_name}")

    # 1) Reference existing delta table
    embeddings_table = DeltaTable.forName(spark, embeddings_table_name)

    # 2) Alias your new data
    sourceDf = data_df.alias("src")

    # 3) Perform the merge
    (
    embeddings_table.alias("tgt").merge(
          sourceDf,
          "tgt.metadata_id = src.metadata_id"
      )
    .whenMatchedDelete()
    .execute()
    )

    (
    embeddings_table.alias("tgt").merge(
          sourceDf,
          "tgt.metadata_id = src.metadata_id"
      )
    .whenNotMatchedInsert(values={
                "metadata_id": "src.metadata_id",
                "chunk_text": "src.chunk_text",
                "document_page_number": "src.document_page_number",
                "document_total_pages": "src.document_total_pages",
                "chunking_status": "src.chunking_status"
            }) 
    .execute()
    )
    
    print("upsert completed")

# COMMAND ----------

def update_metadata(data_df, docs_metadata_table):
    # TODO calculate the partial complete scenarios for each metadata_id
    update_source_df=data_df.select("metadata_id", "chunking_status").distinct()

    print(f"updating metadata table {docs_metadata_table}")

    # 1) Reference existing delta table
    docs_meta_table = DeltaTable.forName(spark, docs_metadata_table)

    # 2) Alias your new data
    sourceDf = update_source_df.alias("src")

    # 3) Perform the merge
    (
    docs_meta_table.alias("tgt")
      .merge(
          sourceDf,
          "tgt.metadata_id = src.metadata_id"
      )
    #    When records already exist, update just the status columns
      .whenMatchedUpdate(
          set = {
            "chunking_status"      : "src.chunking_status",
            "audit__updated_date": "current_timestamp()"
          }
      ).execute()
    )
    print("update completed")

# COMMAND ----------

meta_ids_df=spark.sql(f"select metadata_id from {docs_metadata_table} where chunking_status='PENDING' and extraction_status = 'COMPLETED'")

# COMMAND ----------

meta_ids_list = [row.metadata_id for row in meta_ids_df.select("metadata_id").collect()]

# COMMAND ----------

pdf_text_df=spark.table(doc_pages_table_name).select("metadata_id", "pdf_text").filter(F.col("metadata_id").isin(meta_ids_list))

# COMMAND ----------

data_df=chunk_data(pdf_text_df, embeddings_table_name)

# COMMAND ----------

upsert_chunk_data(data_df, embeddings_table_name)

# COMMAND ----------

updated_meta_df=spark.table(embeddings_table_name).select("metadata_id", "chunking_status").filter(F.col("metadata_id").isin(meta_ids_list))

# COMMAND ----------

update_metadata(updated_meta_df, docs_metadata_table)

# COMMAND ----------

# spark.sql(f"select * from {docs_metadata_table} where doc_control_name != 'Information Technology' ").display()